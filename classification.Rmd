---
title: "Classification of Denials"
author: "Steven P. Sanderson II, MPH"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    highlight: tango
    theme: flatly
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = F,
    warning = F,
    paged.print = FALSE,
    out.width = "100%",
    out.height = "100%"
)
```

```{r lib_load, include=FALSE}
if(!require(pacman)){install.packages("pacman")}
pacman::p_load(
    "tidyverse",
    "healthyR.data",
    "gt",
    "stringr",
    "tidymodels",
    "visdat",
    "skimr",
    "GGally",
    "purrr",
    "DataExplorer",
    "ROSE",
    "tidyquant",
    "ranger",
    "xgboost"
)
```

# 1 Business Understanding

In our example, the goal is to build a classification model to predict an account being denied by the insurance company or not. In particular, the model should learn from data and be able to predict whether account in a service line is going to be denied, given some predictor variables. Hence, we face a supervised learning situation and should use a classification model to predict the categorical outcomes (approved or denied). Furthermore, we use the F1-Score as a performance measure for our classification problem.

Let’s assume that the model’s output will be fed to another analytics system, along with other data. This downstream system will determine whether it is worth investing in a given area or not. The data processing components (also called data pipeline) are shown in the figure below (you can use Google’s architectural templates to draw a data pipeline).

# 2 Data Understanding

In Data Understanding, you:

  + Import data
  + Clean data
  + Format data properly
  + Create new variables
  + Get an overview about the complete data
  + Split data into training and test set using stratified sampling
  + Discover and visualize the data to gain insights

## 2.1 Import Data

First of all, let’s import the data:

```{r data}
df_tbl <- healthyR_data %>%
    select(
      -mrn
      , -visit_id
      , -expected_length_of_stay
      , -length_of_stay_threshold
      , -readmit_expectation
      , -total_amount_due
    ) %>%
    rename(denial_flag = readmit_flag)
```

## 2.2 Clean data

To get a first impression of the data we take a look at the top 4 rows:

```{r head_data}

df_tbl %>% 
  slice_head(n = 4) %>% 
  gt()
```

## 2.3 Format data

Next, we take a look at the data structure and check whether all data formats are correct:

Numeric variables should be formatted as integers (int) or double precision floating point numbers (dbl).

Categorical (nominal and ordinal) variables should usually be formatted as factors (fct) and not characters (chr). Especially, if they don’t have many levels.

```{r glimpse_data}
glimpse(df_tbl)
```

We see that we have two `chr` columns and two `dttm` columns. We will factor the `chr` columns and keep the `dttm` columns for now.

```{r chr_to_fct}
df_fct_tbl <- df_tbl %>%
  mutate(across(where(is.character), as_factor)) %>%
  mutate(denial_flag = as_factor(denial_flag)) %>%
  mutate(los_outlier_flag = as_factor(los_outlier_flag)) %>%
  mutate(service_line = fct_lump_prop(service_line, prop = 0.015))
```

The package `DataExplorer` helps us to explore the data class structure visually:

```{r vis_data}
#vis_dat(df_fct_tbl, warn_large_data = FALSE)

plot_intro(df_fct_tbl)
```

## 2.4 Missing data
Lets check on where the missing values are:

```{r missing_viz}
plot_missing(df_fct_tbl)
```

So we see that the only column that has missing values is `total_payment_amount` from our experience we know that these can be imputed to $0.00.

```{r tot_pay_amt_zero}
df_imp_tbl <- df_fct_tbl %>%
  mutate(total_payment_amount = ifelse(is.na(total_payment_amount), 0, total_payment_amount))
```

```{r svc_line_denials}
df_imp_tbl %>% 
  count(service_line, name = "total_visits")  %>%
  arrange(desc(total_visits)) %>%
  mutate(percent = round(total_visits / sum(total_visits) * 100, 2)) %>%
  mutate(cum_perc = cumsum(percent)) %>%
  gt() %>%
  cols_label(
    service_line   = "Service Line"
    , total_visits = "Visits"
    , percent      = "Percent %"
    , cum_perc     = "Cumulative %"
  ) %>%
  fmt_number(
    columns = c(total_visits),
    suffixing = TRUE
  ) %>%
  cols_align(align = "left", columns = c(service_line))
```

## 2.5 Data overview

```{r skim_dataq}
skim(df_imp_tbl)
```

## 2.5 Data splitting

Now lets take a look at the class balance:

```{r class_balance}
df_imp_tbl %>%
  ggplot(aes(denial_flag, fill = factor(denial_flag))) +
  geom_bar() +
  theme_tq() + 
  scale_fill_tq()
```

We can see that the variable we are trying to predict (denial_flag) is severly imbalanced. This will cause problems when trying to make predictions on this variable so we must even out the sampling. We can do this with the `ROSE` algorithm.

```{r rose_data}
df_balanced_tbl <- ovun.sample(
  denial_flag ~.
  , data = df_imp_tbl
)$data

df_balanced_tbl %>%
  ggplot(aes(denial_flag, fill = factor(denial_flag))) +
  geom_bar() +
  theme_tq() + 
  scale_fill_tq()
```

Now lets create our splits object and our training and testing data sets.

```{r split_object}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible 
set.seed(123)

# Put 80% of the data into the training set 
data_split <- initial_split(df_balanced_tbl, 
                           prop = 0.8, 
                           strata = denial_flag)

# Create dataframes for the two sets:
train_data <- training(data_split) 
test_data  <- testing(data_split)
```

## 2.8 Data Exploration

The point of data exploration is to gain insights that will help you select important variables for your model and to get ideas for feature engineering in the data preparation phase. Usually, data exploration is an iterative process: once you get a prototype model up and running, you can analyze its output to gain more insights and come back to this exploration step. It is important to note that we perform data exploration only with our training data.

### 2.8.1 Create copy of training data

```{r copy_train}
train_tbl <- train_data
```

Lets create a function that will pass in predictors and give us boxplots back.

```{r print_boxplot}
print_boxplot <- function(.y_var){
  
  # convert strings to variable
  y_var <- sym(.y_var) 
 
  # unquote variables using {{}}
  train_tbl %>% 
  ggplot(
    aes(
      x = denial_flag
      , y = {{y_var}}
      , fill = denial_flag
      , color = denial_flag
      )
    ) +
  geom_boxplot(alpha=0.4) 
  
} 
```

### 2.8.2 Obtain Numeric Vars

Obtain all the names we want for our plots:

```{r y_vars}
y_var <-  train_tbl %>% 
  select(where(is.numeric)) %>% 
  variable.names()
```

View plots:

```{r view_print_boxplots}
map(y_var, print_boxplot)
```

<!-- Additionally, we can use the function ggscatmat to create plots with our dependent variable as color column: -->

<!-- ```{r ggscatmat} -->
<!-- train_tbl %>%  -->
<!--   select(where(is.numeric), denial_flag) %>%  -->
<!--   ggscatmat( -->
<!--     color       = "denial_flag" -->
<!--     , corMethod = "spearman" -->
<!--     , alpha     = 0.2 -->
<!--   ) -->
<!-- ``` -->

The function geom_bin2d() creates a heatmap by counting the number of cases in each group, and then mapping the number of cases to each subgroub’s fill. Lets check them out.

```{r print_categorical_heatmap}
print_categorical_heatmap <- function(.y_var){
  
  # convert strings to variable
  y_var <- sym(.y_var) 
 
  # unquote variables using {{}}
  train_tbl %>% 
    ggplot(
      aes(
        x = denial_flag
        , y = {{y_var}}
      )
    ) +
    geom_bin2d() +
    scale_fill_continuous(type = "viridis") +
    theme_tq()
  
}

```

```{r}
y_vars <- train_tbl %>% 
  select(where(is.factor), -denial_flag) %>% 
  variable.names()
```

```{r view_print_cat_heatmap}
map(y_vars, print_categorical_heatmap)
```

# 3 Data Preperation

Data preparation:

  + Handle missing values
  + Fix or remove outliers
  + Feature selection
  + Feature engineering
  + Feature scaling
  + Create a validation set
  
Next, we’ll preprocess our data before training the models. We mainly use the tidymodels packages recipes and workflows for this steps. Recipes are built as a series of optional data preparation steps, such as:

Data cleaning: Fix or remove outliers, fill in missing values (e.g., with zero, mean, median…) or drop their rows (or columns).

Feature selection: Drop the attributes that provide no useful information for the task.

Feature engineering: Discretize continuous features, decompose features (e.g., the weekday from a date variable, etc.), add promising transformations of features (e.g., log(x), sqrt(x), x2 , etc.) or aggregate features into promising new features (like we already did).

Feature scaling: Standardize or normalize features.

We will want to use our recipe across several steps as we train and test our models. To simplify this process, we can use a model workflow, which pairs a model and recipe together.

## 3.1 Data preparation

We have already created our `splits` object `data_split`

```{r splits_object}
data_split
```

## 3.2 Data prepropecessing recipe

The type of data preprocessing is dependent on the data and the type of model being fit. The excellent book “Tidy Modeling with R” provides an appendix with recommendations for baseline levels of preprocessing that are needed for various model functions.

Let’s create a base recipe for all of our classification models. Note that the sequence of steps matter:

  + The recipe() function has two arguments:

    + A formula. Any variable on the left-hand side of the tilde (~) is considered the model outcome (here, denial_flag). On the right-hand side of the tilde are the predictors. Variables may be listed by name (separated by a +), or you can use the dot (.) to indicate all other variables as predictors.

    + The data. A recipe is associated with the data set used to create the model. This will typically be the training set, so data = train_data here.
    
  + update_role(): This step of adding roles to a recipe is optional; the purpose of using it here is that those two variables can be retained in the data but not included in the model. This can be convenient when, after the model is fit, we want to investigate some poorly predicted value. These ID columns will be available and can be used to try to understand what went wrong.

  + step_naomit() removes observations (rows of data) if they contain NA or NaN values. We use skip = TRUE because we don’t want to perform this part to new data so that the number of samples in the assessment set is the same as the number of predicted values (even if they are NA).

Note that instead of deleting missing values we could also easily substitute (i.e., impute) missing values of variables by one of the following methods (using the training set):

  + median
  + mean
  + mode
  + k-nearest neighbors
  + linear model
  + bagged tree models

Take a look at the recipes reference for an overview about all possible imputation methods.

  + step_novel() converts all nominal variables to factors and takes care of other issues related to categorical variables.

  + step_log() will log transform data (since some of our numerical variables are right-skewed). Note that this step can not be performed on negative numbers.

  + step_normalize() normalizes (center and scales) the numeric variables to have a standard deviation of one and a mean of zero. (i.e., z-standardization).

  + step_dummy() converts our factor column ocean_proximity into numeric binary (0 and 1) variables.

Note that this step may cause problems if your categorical variable has too many levels - especially if some of the levels are very infrequent. In this case you should either drop the variable or pool infrequently occurring values into an “other” category with step_other. This steps has to be performed before step_dummy.

  + step_zv(): removes any numeric variables that have zero variance.

  + step_corr(): will remove predictor variables that have large correlations with other predictor variables.
  
We have already used the `ROSE` package to create a balanced data set and used that to make our splits so we can go ahead and create our recipe now.

```{r recipe_object}
denial_rec <- recipe(
  denial_flag ~ .,
  data = train_data
)%>% 
  step_log(total_charge_amount) %>% 
  step_naomit(everything(), skip = TRUE) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_numeric(), -all_outcomes()) %>%
  step_corr(all_numeric_predictors(), threshold = 0.7, method = "spearman")
```

To view the current set of variables and roles, use the summary() function: `summary(housing_rec)`

```{r summary_rec_obj}
summary(denial_rec)
```

If we would like to check if all of our preprocessing steps from above actually worked, we can proceed as follows:

```{r juice_rec_obj}
prepped_data <- 
  denial_rec %>% # use the recipe object
  prep() %>%     # perform the recipe on training data
  juice()        # extract only the pre-processed data.frame 

prepped_data %>% glimpse()
```

## 3.3 Validation set

Remember that we already partitioned our data set into a training set and test set. This lets us judge whether a given model will generalize well to new data. However, using only two partitions may be insufficient when doing many rounds of hyper-parameter tuning (which we don’t perform in this tutorial but it is always recommended to use a validation set).

Therefore, it is usually a good idea to create a so called validation set. We use k-fold crossvalidation to build a set of 5 validation folds with the function vfold_cv. We also use stratified sampling:

```{r vfold_cv}
set.seed(100)

cv_folds <- vfold_cv(
  train_data,
  v      = 5,
  strata = denial_flag
  )
```

# 4 Model building

## 4.1 Specify models

The process of specifying our models is always as follows:

  + Pick a model type
  +Sset the engine
  + Set the mode: regression or classification

### 4.1.1 Logistic regression

```{r logistic_model_spec}
log_spec <- # your model specification
  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# Show your model specification
log_spec
```

### 4.1.2 Random forest

Random Forest from the ranger package

```{r random_forest}
rf_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")
```

When we set the engine, we add importance = "impurity". This will provide variable importance scores for this model, which gives some insight into which predictors drive model performance.

### 4.1.3 Boosted tree (XGBoost)

From xgboost package

```{r xgboost}
xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```

### 4.1.4 K-nearest neighbor

```{r knn}
knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification")
```

4.2 Create workflows
To combine the data preparation recipe with the model building, we use the package workflows. A workflow is an object that can bundle together your pre-processing recipe, modeling, and even post-processing requests (like calculating the RMSE).

4.2.1 Logistic regression
Bundle recipe and model with workflows:

log_wflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(housing_rec) %>%   # use the new recipe
 add_model(log_spec)   # add your model spec

# show object
log_wflow


https://www.kirenz.com/post/2021-02-17-r-classification-tidymodels/#data-understanding