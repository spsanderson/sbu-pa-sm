---
title: "Classification of Denials"
author: "Steven P. Sanderson II, MPH"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    highlight: tango
    theme: flatly
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = F,
    warning = F,
    paged.print = FALSE,
    out.width = "100%",
    out.height = "100%"
)
```

```{r lib_load, include=FALSE}
if(!require(pacman)){install.packages("pacman")}
pacman::p_load(
    "tidyverse",
    "healthyR.data",
    "gt",
    "stringr",
    "tidymodels",
    "visdat",
    "skimr",
    "GGally",
    "purrr",
    "DataExplorer",
    "ROSE",
    "tidyquant"
)
```

# 1 Business Understanding

In our example, the goal is to build a classification model to predict an account being denied by the insurance company or not. In particular, the model should learn from data and be able to predict whether account in a service line is going to be denied, given some predictor variables. Hence, we face a supervised learning situation and should use a classification model to predict the categorical outcomes (approved or denied). Furthermore, we use the F1-Score as a performance measure for our classification problem.

Let’s assume that the model’s output will be fed to another analytics system, along with other data. This downstream system will determine whether it is worth investing in a given area or not. The data processing components (also called data pipeline) are shown in the figure below (you can use Google’s architectural templates to draw a data pipeline).

# 2 Data understanding

In Data Understanding, you:

  + Import data
  + Clean data
  + Format data properly
  + Create new variables
  + Get an overview about the complete data
  + Split data into training and test set using stratified sampling
  + Discover and visualize the data to gain insights

## 2.1 Import Data

First of all, let’s import the data:

```{r data}
df_tbl <- healthyR_data %>%
    select(-mrn, -visit_id, -expected_length_of_stay, -length_of_stay_threshold, 
           -readmit_expectation, -total_amount_due) %>%
    rename(denial_flag = readmit_flag)
```

## 2.2 Clean data

To get a first impression of the data we take a look at the top 4 rows:

```{r head_data}

df_tbl %>% 
  slice_head(n = 4) %>% 
  gt()
```

## 2.3 Format data

Next, we take a look at the data structure and check whether all data formats are correct:

Numeric variables should be formatted as integers (int) or double precision floating point numbers (dbl).

Categorical (nominal and ordinal) variables should usually be formatted as factors (fct) and not characters (chr). Especially, if they don’t have many levels.

```{r glimpse_data}
glimpse(df_tbl)
```

We see that we have two `chr` columns and two `dttm` columns. We will factor the `chr` columns and keep the `dttm` columns for now.

```{r chr_to_fct}
df_fct_tbl <- df_tbl %>%
  mutate(across(where(is.character), as_factor)) %>%
  mutate(service_line = fct_lump_prop(service_line, prop = 0.015))
```

The package `DataExplorer` helps us to explore the data class structure visually:

```{r vis_data}
#vis_dat(df_fct_tbl, warn_large_data = FALSE)

plot_intro(df_fct_tbl)
```

## 2.4 Missing data
Lets check on where the missing values are:

```{r missing_viz}
plot_missing(df_fct_tbl)
```

So we see that the only column that has missing values is `total_payment_amount` from our experience we know that these can be imputed to $0.00.

```{r tot_pay_amt_zero}
df_imp_tbl <- df_fct_tbl %>%
  mutate(total_payment_amount = ifelse(is.na(total_payment_amount), 0, total_payment_amount))
```

```{r svc_line_denials}
df_imp_tbl %>% 
  count(service_line, name = "total_visits")  %>%
  arrange(desc(total_visits)) %>%
  mutate(percent = round(total_visits / sum(total_visits) * 100, 2)) %>%
  mutate(cum_perc = cumsum(percent)) %>%
  gt() %>%
  cols_label(
    service_line   = "Service Line"
    , total_visits = "Visits"
    , percent      = "Percent %"
    , cum_perc     = "Cumulative %"
  ) %>%
  fmt_number(
    columns = c(total_visits),
    suffixing = TRUE
  ) %>%
  cols_align(align = "left", columns = c(service_line))
```

## 2.5 Data overview

```{r skim_dataq}
skim(df_imp_tbl)
```

## 2.5 Data splitting

Now lets take a look at the class balance:

```{r class_balance}
df_imp_tbl %>%
  ggplot(aes(denial_flag, fill = factor(denial_flag))) +
  geom_bar() +
  theme_tq() + 
  scale_fill_tq()
```

We can see that the variable we are trying to predict (denial_flag) is severly imbalanced. This will cause problems when trying to make predictions on this variable so we must even out the sampling. We can do this with the `ROSE` algorithm.

```{r rose_data}
df_balanced_tbl <- ovun.sample(
  denial_flag ~.
  , data = df_imp_tbl
)$data

df_balanced_tbl %>%
  ggplot(aes(denial_flag, fill = factor(denial_flag))) +
  geom_bar() +
  theme_tq() + 
  scale_fill_tq()
```

Now lets create our splits object and our taining and testing data sets.

```{r split_object}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible 
set.seed(123)

# Put 3/4 of the data into the training set 
data_split <- initial_split(df_balanced_tbl, 
                           prop = 0.8, 
                           strata = denial_flag)

# Create dataframes for the two sets:
train_data <- training(data_split) 
test_data <- testing(data_split)
```