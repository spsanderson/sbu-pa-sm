---
title: "Classification of Denials"
author: "Steven P. Sanderson II, MPH"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: tango
    theme: flatly
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = F,
    warning = F,
    paged.print = FALSE,
    out.width = "100%",
    out.height = "100%"
)
```

```{r lib_load, include=FALSE}
if(!require(pacman)){install.packages("pacman")}
pacman::p_load(
    "tidyverse",
    "healthyR.data",
    "gt",
    "stringr",
    "tidymodels",
    "visdat",
    "skimr",
    "GGally",
    "purrr"
)
```

```{r data, include=FALSE}
df_tbl <- healthyR_data %>%
    select(-mrn, -visit_id, -expected_length_of_stay, -length_of_stay_threshold, 
           -readmit_expectation) %>%
    rename(denial_flag = readmit_flag)
```


# Business Understanding

In our example, the goal is to build a classification model to predict an account being denied by the insurance company or not. In particular, the model should learn from data and be able to predict whether account in a service line is going to be denied, given some predictor variables. Hence, we face a supervised learning situation and should use a classification model to predict the categorical outcomes (approved or denied). Furthermore, we use the F1-Score as a performance measure for our classification problem.

Let’s assume that the model’s output will be fed to another analytics system, along with other data. This downstream system will determine whether it is worth investing in a given area or not. The data processing components (also called data pipeline) are shown in the figure below (you can use Google’s architectural templates to draw a data pipeline).