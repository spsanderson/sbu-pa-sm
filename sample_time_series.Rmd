---
title: "Sample Time Series"
author: "Steven P. Sanderson II, MPH"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: tango
    theme: flatly
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = F,
    warning = F,
    paged.print = FALSE,
    out.width = "100%",
    out.height = "100%"
)
```

```{r run_main_script, include=FALSE, echo=FALSE, warning=FALSE}
source("00_Scripts/main.R")
```

# Purpose

The purpose of this document is to illustrate time series analysis and forecasting. We will use a simulated dataset to analyze things like visits, discharges and payments. To perform these analyses we will be following the `modeltime` workflow. This report will be broken down into sections that follow that same workflow.

# Data View

Lets take a look at our data and see what it has.

```{r data_view}
df_tbl %>%
  glimpse()
```

```{r skimr}
skim(df_tbl)
```

# Preparing Data

Our objectives are to:

- Aggregate data to common time-stamps
- Apply any transformations
- Detect any lags & add rolling features
- Create a Full Data Set: Adding Future observations, lags, and external regressors

Our forecasting will focus on a grouped forecast where we are going to forecast the number of discharges by inpatient/outpatient visit type and by payer grouping.

We are going to do this on a weekly scale.

## Aggregate discharges by IP/OP and Payer Grouping by Week

1. Start with `df_tbl`
2. Use `summarise_by_time()` with `.by = "week"`, and `n()` the visits.
3. Save as a new variable called `transactions_weekly_tbl`

```{r transactions_weekly_tbl}
transactions_weekly_tbl <- df_tbl %>%
  filter(payer_grouping != "?") %>%
  group_by(ip_op_flag, payer_grouping) %>%
  summarise_by_time(
    .date_var = dsch_date
    , .by     = "week"
    , value   = n()
  )

transactions_weekly_tbl
```

# Visualizations

## Visualize Discharges

Use `plot_time_series()` to visualize the discharges. 

- Look for outliers & any data issues
- Try out a `log()` transformation to see the effect on the time series

```{r weekly_ts}
transactions_weekly_tbl %>%
  plot_time_series(
    .date_var     = dsch_date
    , .color_var  = payer_grouping
    , .facet_vars = payer_grouping
    , .facet_ncol = 2
    , .value      = log(value)
    , .smooth     = FALSE
  )
```

## Visualize ACF

Visualize the ACF using `plot_acf_diagnostics()` using a `log()` transformation. Look for:

- Any frequencies we can include?
- Any lags we can include? (Hint - What is our forecast horizon?)

```{r}
transactions_weekly_tbl %>%
  ungroup() %>%
  plot_acf_diagnostics(dsch_date, log(value))
```

## Log-Standardize Revenue (Target)

- Start with `transactions_weekly_tbl`
- Apply log-standardization:
    - Apply Log transformation using `log()`
    - Apply standardization to mean = 0, sd = 1 using `standardize_vec()`
- Store the resulting data as `transactions_trans_weekly_tbl`

```{r, message = TRUE}
transactions_trans_weekly_tbl <- transactions_weekly_tbl %>%
  mutate(value = log(value)) %>%
  mutate(value = standardize_vec(value))

mean_a <- 3.08875144281386
sd_a   <- 0.367674566335952
mean_b <- 1.83577890003612
sd_b   <- 0.545791389303644
mean_c <- 3.15330156564258
sd_c   <- 0.302421031976675
mean_d <- 1.59951348649452
sd_d   <- 0.514947645076106
```

Visualize the log-standardized transactions using `plot_time_series()`. This confirms the transformation was performed successfully. 

```{r}
transactions_trans_weekly_tbl %>%
      plot_time_series(
    .date_var     = dsch_date
    , .color_var  = payer_grouping
    , .facet_vars = payer_grouping
    , .facet_ncol = 2
    , .value      = value
    , .smooth     = FALSE
  )
```

We'll use these parameters to create our "full dataset". We've selected an 14-week forecast horizon. Our lag period is 14 weeks and we'll try out a few rolling averages at various aggregations. 

```{r}
horizon         <- 14
lag_period      <- 14
rolling_periods <- c(7, 14, 28, 52)
```

## Prepare the full data

1. Start with `transactions_weekly_tbl`
2. __Add the future window:__ Use `bind_rows()` and `future_frame()` to extend the data frame `.length_out = horizon`.
3. __Add autocorrelated lags:__ Use `tk_augment_lags()` to add a `.lags = lag_period`
4. __Add rolling features from our lag__: Use `tk_agument_slidify()` to add `.period = rolling_periods`. Use `mean` as the rolling function. Make sure to "center" with "partial" windows. 
5. Rename any columns that contain "lag". Modify to start with "lag_"
6. Save the output as `full_tbl`.


```{r}
full_tbl <- transactions_trans_weekly_tbl %>%
    
    # Add future window
    bind_rows(
        future_frame(
          .data         = .
          , .date_var   = dsch_date
          , .length_out = horizon
        )
    ) %>%
    
    # Add autocorrelated lags
    tk_augment_lags(value, .lags = lag_period) %>%
    
    # Add rolling features
    tk_augment_slidify(
        .value   = value_lag14,
        .f       = mean, 
        .period  = rolling_periods,
        .align   = "center",
        .partial = TRUE
    ) %>%
    
    # Rename columns
    rename_with(
      .cols = contains("lag")
      , .fn = ~ str_c("lag_", .)
    ) %>%
  select(dsch_date, everything())

full_tbl %>% 
  glimpse()
```

## Visualize the Full Data

Visualize the features, and review what you see. 

1. Start with `full_tbl`
2. `pivot_longer` every column except "dsch_date"
3. Use `plot_time_series()` to visualize the time series coloring by "name". 

Review the visualization selecting one feature at a time and answering the following questions:
    
    - Do the rolling lags present any issues? 
    - Which rolling lag captures the trend the best?
    - Do you expect either of the Product Events features to help?

```{r}
full_tbl %>%
  pivot_longer(cols = -c(dsch_date, lag_ip_op_flag, payer_grouping)) %>%
  plot_time_series(
    dsch_date
    , value
    , name
    , .smooth = FALSE
    , .facet_ncol = 2
  )
```

# Model Data / Forecast Data Split

Create a `data_prepared_tbl` by filtering `full_tbl` where "value" is non-missing. 

```{r}
data_prepared_tbl <- full_tbl %>%
    filter(!is.na(value))
data_prepared_tbl
```

Create a `forecast_tbl` by filtering `full_tbl` where "value" is missing. 

```{r}
forecast_tbl <- full_tbl %>%
    filter(is.na(value))
forecast_tbl
```

# Train / Test Split

## Split into Train / Test Sets

- Start with `data_prepared_tbl`
- Use `time_series_split()` to create a single time series split. 
    - Set `assess = horizon` to get the last 14-weeks of data as testing data. 
    - Set `cumulative = TRUE` to use all of the previous data as training data. 
- Save the object as `splits`

```{r}
splits <- data_prepared_tbl %>% 
    time_series_split(assess = horizon, cumulative = TRUE)
```

# Feature Engineering

## Create a Preprocessing recipe

Make a preprocessing recipe using `recipe()`. Note - It may help to `prep()` and `juice()` your recipe to see the effect of your transformations. 

- Start with `recipe()` using "value ~ ." and `data = training(splits)`
- Add the following steps:
    - `step_timeseries_signature()` using the date feature
    - Remove any newly created features that:
        - Contain ".iso"
        - End with "xts"
        - Contain "day", "hour", "minute", "second" or "am.pm" (because this is a weekly dataset and these features won't add any predictive value)
    - Normalize all numeric data except for "value" (the target) with `step_normalize()`.
    - Dummy all categorical features with `step_dummy()`. Set `one_hot = TRUE`.
    - Add a fourier series at periods 7 and 14. Set K = 2 for both. 

```{r}
recipe_spec_base <- recipe(
  value ~ .
  , data = training(splits) %>%
    arrange(
      lag_ip_op_flag
      , payer_grouping
      , dsch_date)
  ) %>%
    
    # Time Series Signature
    step_timeseries_signature(dsch_date) %>%
    step_rm(matches("(iso)|(xts)|(hour)|(minute)|(second)|(am.pm)")) %>%
    
    # Standardization
    step_normalize(matches("(index.num)|(year)|(yday)")) %>%
    
    # Dummy Encoding (One Hot Encoding)
    step_dummy(all_nominal(), one_hot = TRUE) %>%
    
    # Fourier - 7 Week ACF
    step_fourier(dsch_date, period = c(7, 14, 52), K = 2)

recipe_spec_base %>% 
  prep() %>% 
  juice() %>% 
  glimpse()
```

# Modeling

## Spline Model

### Visualize

Use `plot_time_series_regression` to test out several natural splines:

- Use .formula to try out `splines::ns()` with degrees of freedom 1, 2, 3, and 4. 

Which value of `df` would you select?

```{r}
data_prepared_tbl %>%
    plot_time_series_regression(
        .date_var     = dsch_date,
        .formula      = value ~ splines::ns(dsch_date, df = 3),
        .show_summary = FALSE,
        .facet_ncol   = 2
    )
```

### LM Model Spec

Create a model specification for linear regression:

- Use `linear_reg()` function
- Use `set_engine("lm")`
- Store as `model_spec_lm`

```{r}
model_spec_lm <- linear_reg() %>%
    set_engine("lm")
```

### Recipe Spec - Spline

Create a recipe for the spline model. 

1. Start with `recipe_spec_base`
2. Add a step to remove the "dsch_date" feature. We don't need this for LM models. 
3. Add a step for the natural spline. Set `deg_free = 3`
4. Remove any features that begin with "lag_"
5. Store your updated recipe as `recipe_spec_1_spline`
6. Glimpse the output. Were the features adjusted correctly?

```{r}
recipe_spec_1_spline <- recipe_spec_base %>%
    step_rm(dsch_date) %>%
    step_ns(ends_with("index.num"), deg_free = 3) %>%
    step_rm(starts_with("lag_"))

recipe_spec_1_spline %>% 
  prep() %>% 
  juice() %>% 
  glimpse()
```

### Workflow - Spline

Create a workflow for the linear regression and preprocessing recipe:

- Start with a `workflow()`
- Use `add_model()` to add the `model_spec_lm`
- Use `add_recipe()` to add the `recipe_spec_1_spline`
- Store as `workflow_fit_lm_1_spline`


```{r}
workflow_fit_lm_1_spline <- workflow() %>%
    add_model(model_spec_lm) %>%
    add_recipe(recipe_spec_1_spline) %>%
    fit(
      training(splits) %>%
        arrange(
          lag_ip_op_flag
          , payer_grouping
          , dsch_date)
      )

workflow_fit_lm_1_spline %>% 
  pull_workflow_fit() %>% 
  pluck("fit") %>% 
  summary()
```

## Rolling Lag Model

### Recipe Spec - Lag

Create a recipe for the spline model. 

1. Start with `recipe_spec_base`
2. Add a step to remove the "purchased_at" feature. We don't need this for LM models. 
3. Remove missing values in any column that starts with "lag_"
4. Store your updated recipe as `recipe_spec_2_lag`
5. Glimpse the output. Were the features adjusted correctly?

```{r}
recipe_spec_2_lag <- recipe_spec_base %>%
    step_rm(dsch_date) %>%
    step_naomit(starts_with("lag_"))

recipe_spec_2_lag %>% 
  prep() %>% 
  juice() %>% 
  glimpse()
```

### Workflow - Lag

Save the workflow as `workflow_fit_lm_2_lag`.

```{r}
workflow_fit_lm_2_lag <- workflow() %>%
    add_model(model_spec_lm) %>%
    add_recipe(recipe_spec_2_lag) %>%
    fit(
      training(splits)  %>%
        arrange(
          lag_ip_op_flag
          , payer_grouping
          , dsch_date
        )
    )

workflow_fit_lm_2_lag %>% 
  pull_workflow_fit() %>% 
  pluck("fit") %>% 
  summary()
```

# Modeltime

### Make a Modeltime Table

Start by making a modeltime table:

- Use `modeltime_table()` to store your fitted workflows
- Save as `model_tbl`

```{r}
model_tbl <- modeltime_table(
    workflow_fit_lm_1_spline,
    workflow_fit_lm_2_lag
)
model_tbl
```

As a precautionary measure, please refit the models using `modeltime_refit()`. This prevents models that can go bad over time because of software changes. 

```{r}
# Refitting makes sure your models work over time. 
model_tbl <- model_tbl %>%
    modeltime_refit(training(splits)  %>%
        arrange(
          lag_ip_op_flag
          , payer_grouping
          , dsch_date
        ))
```

### Calibrate the Table

Use testing data to calibrate the model:

- Start with `model_tbl`
- Use `modeltime_calibrate()` to calibrate the model using `testing(splits)` (out-of-sample data)
- Store the result as `calibration_tbl`

```{r}
calibration_tbl <- model_tbl %>%
    modeltime_calibrate(
      training(splits)  %>%
        arrange(
          lag_ip_op_flag
          , payer_grouping
          , dsch_date
        )
    )
calibration_tbl
```

### Calculate the Accuracy

Use `modeltime_accuracy()` to calculate the accuracy metrics.

```{r}
calibration_tbl %>% modeltime_accuracy()
```

### Visualize the Model Forecast

- Use `modeltime_forecast()`:
    - Set `new_data = testing(splits)`
    - Set `actual_data = data_prepared_tbl`
- Pipe the result into `plot_modeltime_forecast()`

```{r}
calibration_tbl %>%
    modeltime_forecast(
        new_data    = testing(splits) %>%
        arrange(
          lag_ip_op_flag
          , payer_grouping
          , dsch_date
        ),
        actual_data = data_prepared_tbl
    ) %>%
    plot_modeltime_forecast()
```


Forecasting thoughts:

- What can you say about the Rolling Lag Model?
- What might we be able to do to correct the model? (HINT: Try removing features in the lag model - what happens?)

# Forecast Future Data

## Refit the Model

- Start with the `calibration_tbl`
- Use `modeltime_refit()` refit the model on the `data_prepared_tbl` dataset

```{r}
refit_tbl <- calibration_tbl %>%
    modeltime_refit(data = data_prepared_tbl)
```


## Forecast

1. Start with `refit_tbl`
2. Use `modeltime_forecast()` to forecast the `new_data = forecast_tbl`. Use `data_prepared_tbl` as the actual data. 
3. Plot the forecast using `plot_modeltime_forecast()`

```{r}
refit_tbl %>%
    modeltime_forecast(new_data    = forecast_tbl,
                       actual_data = data_prepared_tbl) %>%
    
    
    plot_modeltime_forecast()
```

## Invert Transformation

Apply the inversion to the forecast plot:

- Invert the standardization
- Invert the log transformation

```{r}
refit_tbl %>%
    modeltime_forecast(new_data    = forecast_tbl,
                       actual_data = data_prepared_tbl) %>%
    
    # Invert Transformation
    mutate(across(.value:.conf_hi, .fns = ~ standardize_inv_vec(
        x    = .,
        mean = c(mean_a, mean_b, mean_c, mean_d),
        sd   = c(sd_a, sd_b, sd_c, sd_d)
    ))) %>%
    mutate(across(.value:.conf_hi, .fns = exp)) %>%
    
    plot_modeltime_forecast()
```




# Forecast Review

## GLMNet - Elastic Net 

```{r}
workflow_fit_glmnet_2_lag <- workflow_fit_lm_2_lag %>%
    update_model(
        spec = linear_reg(penalty = 0.1, mixture = 0.5) %>%
            set_engine("glmnet")
    ) %>%
    fit(training(splits)%>%
        arrange(
          lag_ip_op_flag
          , payer_grouping
          , dsch_date
        ))
```

```{r}
calibration_tbl <- modeltime_table(
    workflow_fit_lm_1_spline,
    workflow_fit_lm_2_lag,
    workflow_fit_glmnet_2_lag
) %>%
    
    update_model_description(.model_id = 1, "LM - Spline Recipe") %>%
    update_model_description(2, "LM - Lag Recipe") %>%
    update_model_description(3, "GLMNET - Lag Recipe") %>%
    
    modeltime_calibrate(testing(splits)%>%
        arrange(
          lag_ip_op_flag
          , payer_grouping
          , dsch_date
        ))
```

```{r}
calibration_tbl %>% modeltime_accuracy()
```

```{r}
calibration_tbl %>%
    modeltime_forecast(
        new_data = testing(splits)%>%
        arrange(
          lag_ip_op_flag
          , payer_grouping
          , dsch_date
        ),
        actual_data = data_prepared_tbl
    ) %>%
    plot_modeltime_forecast()
```

```{r}
refit_tbl <- calibration_tbl %>%
    modeltime_refit(data = data_prepared_tbl)
```

```{r}
refit_tbl %>%
    modeltime_forecast(
        new_data = forecast_tbl,
        actual_data = data_prepared_tbl
    ) %>%
    plot_modeltime_forecast()
```